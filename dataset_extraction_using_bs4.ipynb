{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs8VxdAFeILzooQUTO6b+q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushigupta20/Projects/blob/main/dataset_extraction_using_bs4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkRvtDCqW919",
        "outputId": "766dba8e-fecd-4f0e-b040-1c7c9397dc2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Function to fetch PubMed Central IDs for articles matching the query\n",
        "def fetch_pubmed_central_data(query, max_results=500):\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    id_list = []\n",
        "    for start in range(0, max_results, 100):\n",
        "        params = {\n",
        "            \"db\": \"pmc\",\n",
        "            \"term\": query,\n",
        "            \"retstart\": start,\n",
        "            \"retmax\": min(100, max_results - start),\n",
        "            \"mindate\": \"2020/01/01\",\n",
        "            \"sort\": \"relevance\"\n",
        "        }\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            root = ET.fromstring(response.content)\n",
        "            id_list.extend([id_elem.text for id_elem in root.findall('.//Id')])\n",
        "        else:\n",
        "            print(\"Error fetching data from PubMed Central\")\n",
        "            break\n",
        "    return id_list\n",
        "\n",
        "# Function to fetch detailed information for articles given a list of PubMed Central IDs\n",
        "def fetch_article_details(id_list):\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    articles = []\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(id_list), batch_size):\n",
        "        batch_ids = id_list[i:i+batch_size]\n",
        "        params = {\n",
        "            \"db\": \"pmc\",\n",
        "            \"id\": \",\".join(batch_ids),\n",
        "            \"retmode\": \"xml\"\n",
        "        }\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            articles.append(response.content)\n",
        "        else:\n",
        "            print(f\"Error fetching article details for batch starting at index {i}\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "            response = requests.get(base_url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                articles.append(response.content)\n",
        "            else:\n",
        "                print(f\"Failed again fetching article details for batch starting at index {i}\")\n",
        "    return articles\n",
        "\n",
        "# Function to parse article details and extract necessary information\n",
        "def parse_article_details(article_details, id_list):\n",
        "    articles = []\n",
        "    for content in article_details:\n",
        "        root = ET.fromstring(content)\n",
        "        for i, article in enumerate(root.findall(\".//article\")):\n",
        "            title_elem = article.find(\".//article-title\")\n",
        "            title = title_elem.text.strip() if title_elem is not None and title_elem.text is not None else \"\"\n",
        "            link = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{id_list[i]}\"\n",
        "            article_type = None\n",
        "            article_meta_elem = article.find(\".//article-meta\")\n",
        "            if article_meta_elem is not None:\n",
        "                pub_type_elem = article_meta_elem.find(\".//article-categories/subj-group/subject\")\n",
        "                if pub_type_elem is not None:\n",
        "                    article_type = pub_type_elem.text.strip()\n",
        "            supplementary_datasets = fetch_supplementary_materials(id_list[i])\n",
        "            articles.append({\"Title\": title, \"Link\": link, \"ArticleType\": article_type, \"SupplementaryDatasets\": supplementary_datasets})\n",
        "    return articles\n",
        "\n",
        "# Function to check for availability of supplementary materials\n",
        "def fetch_supplementary_materials(article_id, retries=3):\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    params = {\n",
        "        \"db\": \"pmc\",\n",
        "        \"id\": article_id,\n",
        "        \"retmode\": \"xml\"\n",
        "    }\n",
        "    for attempt in range(retries):\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            return parse_supplementary_materials(response.content, article_id)\n",
        "        else:\n",
        "            print(f\"Error fetching supplementary materials for article {article_id}, attempt {attempt + 1}\")\n",
        "            time.sleep(2)  # Wait for 5 seconds before retrying\n",
        "    return []\n",
        "\n",
        "# Function to parse supplementary materials\n",
        "def parse_supplementary_materials(article_details, article_id):\n",
        "    root = ET.fromstring(article_details)\n",
        "    datasets = []\n",
        "    for article in root.findall(\".//sec[@sec-type='supplementary-material']/sec\"):\n",
        "        link_elems = article.findall(\".//media\")\n",
        "        for link_elem in link_elems:\n",
        "            dataset_link = link_elem.get(\"{http://www.w3.org/1999/xlink}href\", \"\")\n",
        "            if dataset_link:\n",
        "                full_link = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{article_id}/bin/{dataset_link.split('/')[-1]}\"\n",
        "                datasets.append(full_link)\n",
        "    return datasets\n",
        "\n",
        "# Function to save articles to a CSV file\n",
        "def save_to_csv(articles, filename=\"pmc_results.csv\"):\n",
        "    with open(filename, \"w\", newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = [\"Title\", \"Link\", \"ArticleType\", \"SupplementaryDatasets\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for article in articles:\n",
        "            article['SupplementaryDatasets'] = \", \".join(article['SupplementaryDatasets'])\n",
        "            writer.writerow(article)\n",
        "\n",
        "# Main function to handle the workflow\n",
        "def main():\n",
        "    query = input(\"Enter your query: \")\n",
        "    max_results = 500  # Fetch 500 results\n",
        "\n",
        "    print(f\"Fetching data for query: {query}\")\n",
        "    id_list = fetch_pubmed_central_data(query, max_results)\n",
        "    print(f\"Found {len(id_list)} articles.\")\n",
        "\n",
        "    if id_list:\n",
        "        article_details = fetch_article_details(id_list)\n",
        "        if article_details:\n",
        "            articles = parse_article_details(article_details, id_list)\n",
        "            save_to_csv(articles)\n",
        "            print(\"Results saved to pmc_results.csv\")\n",
        "\n",
        "            df = pd.read_csv(\"pmc_results.csv\")\n",
        "            # Filtering out rows with empty SupplementaryDatasets column\n",
        "            df_filtered = df.dropna(subset=['SupplementaryDatasets'])\n",
        "            df_filtered.to_csv(\"filtered_file.csv\", index=False)\n",
        "            print(\"Filtered file saved successfully\")\n",
        "\n",
        "    else:\n",
        "        print(\"No articles found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GojtnyPZXE9K",
        "outputId": "3e76c23f-82b4-4f58-c5c1-7fc8e2b51457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: (\"Breast Neoplasms/prevention and control\"[Mesh]) AND \"Adolescent\"[Mesh]\n",
            "Fetching data for query: (\"Breast Neoplasms/prevention and control\"[Mesh]) AND \"Adolescent\"[Mesh]\n",
            "Found 124 articles.\n",
            "Results saved to pmc_results.csv\n",
            "Filtered file saved successfully\n"
          ]
        }
      ]
    }
  ]
}